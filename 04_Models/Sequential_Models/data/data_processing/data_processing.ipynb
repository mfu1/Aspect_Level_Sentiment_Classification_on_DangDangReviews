{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "import json\n",
    "import string\n",
    "import codecs\n",
    "import jieba\n",
    "\n",
    "import torch\n",
    "from pytorch_transformers import *\n",
    "\n",
    "from zhon import hanzi\n",
    "from collections import Counter\n",
    "\n",
    "import xml.etree.ElementTree as ET, getopt, logging, sys, random, re, copy\n",
    "from xml.sax.saxutils import escape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pytorch_transformers.tokenization_utils:Model name '../data/embeddings/BERT/bert-base-chinese/bert-base-chinese-vocab.txt' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc). Assuming '../data/embeddings/BERT/bert-base-chinese/bert-base-chinese-vocab.txt' is a path or url to a directory containing tokenizer files.\n",
      "INFO:pytorch_transformers.tokenization_utils:loading file ../data/embeddings/BERT/bert-base-chinese/bert-base-chinese-vocab.txt\n",
      "INFO:pytorch_transformers.tokenization_utils:loading file ../data/embeddings/BERT/bert-base-chinese/bert-base-chinese-vocab.txt\n",
      "INFO:pytorch_transformers.tokenization_utils:loading file ../data/embeddings/BERT/bert-base-chinese/bert-base-chinese-vocab.txt\n"
     ]
    },
    {
     "ename": "JSONDecodeError",
     "evalue": "Expecting value: line 1 column 2 (char 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-457-b64795757cee>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mBERT_PATH\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;34m'../data/embeddings/BERT/bert-base-chinese'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mBERT_VOCAB\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;34m'bert-base-chinese-vocab.txt'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBertTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBERT_PATH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBERT_VOCAB\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBertModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBERT_PATH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pytorch_transformers/tokenization_bert.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    198\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'do_lower_case'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBertTokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_from_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    201\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pytorch_transformers/tokenization_utils.py\u001b[0m in \u001b[0;36m_from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, cache_dir, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    232\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0margs_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfile_path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mspecial_tokens_map_file\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m             \u001b[0mspecial_tokens_map\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspecial_tokens_map_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mspecial_tokens_map\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/json/__init__.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(fp, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    297\u001b[0m         \u001b[0mcls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobject_hook\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobject_hook\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m         \u001b[0mparse_float\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparse_float\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparse_int\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparse_int\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 299\u001b[0;31m         parse_constant=parse_constant, object_pairs_hook=object_pairs_hook, **kw)\n\u001b[0m\u001b[1;32m    300\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/json/__init__.py\u001b[0m in \u001b[0;36mloads\u001b[0;34m(s, encoding, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    352\u001b[0m             \u001b[0mparse_int\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mparse_float\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m             parse_constant is None and object_pairs_hook is None and not kw):\n\u001b[0;32m--> 354\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_default_decoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    355\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0mcls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mJSONDecoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/json/decoder.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    337\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m         \"\"\"\n\u001b[0;32m--> 339\u001b[0;31m         \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    340\u001b[0m         \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/json/decoder.py\u001b[0m in \u001b[0;36mraw_decode\u001b[0;34m(self, s, idx)\u001b[0m\n\u001b[1;32m    355\u001b[0m             \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscan_once\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 357\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mJSONDecodeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Expecting value\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    358\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mJSONDecodeError\u001b[0m: Expecting value: line 1 column 2 (char 1)"
     ]
    }
   ],
   "source": [
    "BERT_PATH ='../data/embeddings/BERT/bert-base-chinese'\n",
    "BERT_VOCAB ='bert-base-chinese-vocab.txt'\n",
    "tokenizer = BertTokenizer.from_pretrained(os.path.join(BERT_PATH, BERT_VOCAB))\n",
    "model = BertModel.from_pretrained(BERT_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = torch.tensor([tokenizer.encode(\"Here is some text to encode\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[10816,  8311, 13049, 10540,  8229,  9448, 10803]])"
      ]
     },
     "execution_count": 452,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    last_hidden_states = model(input_ids)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.7559, -0.8967,  0.4671,  ...,  0.8768, -0.3557,  0.0315],\n",
       "         [ 0.0102, -0.9331, -0.1013,  ..., -0.0526, -0.3079,  0.1158],\n",
       "         [ 0.5664,  0.0461,  0.1432,  ...,  0.2052, -0.4931,  0.0084],\n",
       "         ...,\n",
       "         [-0.1225, -0.4007,  0.1629,  ..., -0.3553, -0.5007, -0.0392],\n",
       "         [ 0.0384, -0.2626, -0.0595,  ..., -0.2261, -0.2866,  0.2040],\n",
       "         [ 0.5811, -0.1267, -0.2250,  ..., -0.1337, -0.3257,  0.3426]]])"
      ]
     },
     "execution_count": 454,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DATA_DIR_FROM = \"../data/data_orign/SemEval2015\"\n",
    "DATA_DIR_TO = \"../data/data_processed/SemEval2015\"\n",
    "\n",
    "STOPWORDS = codecs.open(\"Frequent_Stopwords_ZH.txt\", encoding=\"utf-8\")\\\n",
    "                  .read()\\\n",
    "                  .replace(\" \", \"\")\\\n",
    "                  .split(\",\")\n",
    "PUNCTUATIONS = set(hanzi.punctuation + string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SENTIMENT_IDX = {'negative': 0, 'positive': 1}\n",
    "IDX_SENTIMENT = {0: 'negative', 1: 'positive'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-5-a04fdb8683c2>, line 126)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-5-a04fdb8683c2>\"\u001b[0;36m, line \u001b[0;32m126\u001b[0m\n\u001b[0;31m    print '%d instances\\n%d distinct aspect terms' % (len(self.corpus), len(self.top_aspect_terms))\u001b[0m\n\u001b[0m                                                 ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "class Category:\n",
    "    '''Category objects contain the term and polarity (i.e., pos, neg, neu, conflict) of the category (e.g., food, price, etc.) of a sentence.'''\n",
    "\n",
    "    def __init__(self, term='', polarity=''):\n",
    "        self.term = term\n",
    "        self.polarity = polarity\n",
    "\n",
    "    def create(self, element):\n",
    "        self.term = element.attrib['category']\n",
    "        self.polarity = element.attrib['polarity']\n",
    "        return self\n",
    "\n",
    "    def update(self, term='', polarity=''):\n",
    "        self.term = term\n",
    "        self.polarity = polarity\n",
    "\n",
    "\n",
    "class Aspect:\n",
    "    '''Aspect objects contain the term (e.g., battery life) and polarity (i.e., pos, neg, neu, conflict) of an aspect.'''\n",
    "\n",
    "    def __init__(self, term, polarity, offsets):\n",
    "        self.term = term\n",
    "        self.polarity = polarity\n",
    "        self.offsets = offsets\n",
    "\n",
    "    def create(self, element):\n",
    "        self.term = element.attrib['term']\n",
    "        self.polarity = element.attrib['polarity']\n",
    "        self.offsets = {'from': str(element.attrib['from']), 'to': str(element.attrib['to'])}\n",
    "        return self\n",
    "\n",
    "    def update(self, term='', polarity=''):\n",
    "        self.term = term\n",
    "        self.polarity = polarity\n",
    "\n",
    "\n",
    "class Instance:\n",
    "    '''An instance is a sentence, modeled out of XML (pre-specified format, based on the 4th task of SemEval 2014).\n",
    "    It contains the text, the aspect terms, and any aspect categories.'''\n",
    "\n",
    "    def __init__(self, element):\n",
    "        self.text = element.find('text').text\n",
    "        self.id = element.get('id')\n",
    "        self.aspect_terms = [Aspect('', '', offsets={'from': '', 'to': ''}).create(e) for es in\n",
    "                             element.findall('aspectTerms') for e in es if\n",
    "                             es is not None]\n",
    "        self.aspect_categories = [Category(term='', polarity='').create(e) for es in element.findall('aspectCategories')\n",
    "                                  for e in es if\n",
    "                                  es is not None]\n",
    "\n",
    "    def get_aspect_terms(self):\n",
    "        return [a.term.lower() for a in self.aspect_terms]\n",
    "\n",
    "    def get_aspect_categories(self):\n",
    "        return [c.term.lower() for c in self.aspect_categories]\n",
    "\n",
    "    def add_aspect_term(self, term, polarity='', offsets={'from': '', 'to': ''}):\n",
    "        a = Aspect(term, polarity, offsets)\n",
    "        self.aspect_terms.append(a)\n",
    "\n",
    "    def add_aspect_category(self, term, polarity=''):\n",
    "        c = Category(term, polarity)\n",
    "        self.aspect_categories.append(c)\n",
    "\n",
    "\n",
    "class Corpus:\n",
    "    '''A corpus contains instances, and is useful for training algorithms or splitting to train/test files.'''\n",
    "\n",
    "    def __init__(self, elements):\n",
    "        self.corpus = [Instance(e) for e in elements]\n",
    "        self.size = len(self.corpus)\n",
    "        self.aspect_terms_fd = fd([a for i in self.corpus for a in i.get_aspect_terms()])\n",
    "        self.top_aspect_terms = freq_rank(self.aspect_terms_fd)\n",
    "        self.texts = [t.text for t in self.corpus]\n",
    "\n",
    "    def echo(self):\n",
    "        print '%d instances\\n%d distinct aspect terms' % (len(self.corpus), len(self.top_aspect_terms))\n",
    "        print 'Top aspect terms: %s' % (', '.join(self.top_aspect_terms[:10]))\n",
    "\n",
    "    def clean_tags(self):\n",
    "        for i in range(len(self.corpus)):\n",
    "            self.corpus[i].aspect_terms = []\n",
    "\n",
    "    def split(self, threshold=0.8, shuffle=False):\n",
    "        '''Split to train/test, based on a threshold. Turn on shuffling for randomizing the elements beforehand.'''\n",
    "        clone = copy.deepcopy(self.corpus)\n",
    "        if shuffle: random.shuffle(clone)\n",
    "        train = clone[:int(threshold * self.size)]\n",
    "        test = clone[int(threshold * self.size):]\n",
    "        return train, test\n",
    "\n",
    "    def write_out(self, filename, instances, short=True):\n",
    "        with open(filename, 'w') as o:\n",
    "            o.write('<sentences>\\n')\n",
    "            for i in instances:\n",
    "                o.write('\\t<sentence id=\"%s\">\\n' % (i.id))\n",
    "                o.write('\\t\\t<text>%s</text>\\n' % fix(i.text))\n",
    "                o.write('\\t\\t<aspectTerms>\\n')\n",
    "                if not short:\n",
    "                    for a in i.aspect_terms:\n",
    "                        o.write('\\t\\t\\t<aspectTerm term=\"%s\" polarity=\"%s\" from=\"%s\" to=\"%s\"/>\\n' % (\n",
    "                            fix(a.term), a.polarity, a.offsets['from'], a.offsets['to']))\n",
    "                o.write('\\t\\t</aspectTerms>\\n')\n",
    "                o.write('\\t\\t<aspectCategories>\\n')\n",
    "                if not short:\n",
    "                    for c in i.aspect_categories:\n",
    "                        o.write('\\t\\t\\t<aspectCategory category=\"%s\" polarity=\"%s\"/>\\n' % (fix(c.term), c.polarity))\n",
    "                o.write('\\t\\t</aspectCategories>\\n')\n",
    "                o.write('\\t</sentence>\\n')\n",
    "            o.write('</sentences>')\n",
    "\n",
    "    def write_out_json(self, filename, instances):\n",
    "        fr_to = open(filename, 'w')\n",
    "        ans = []\n",
    "        for i in instances:\n",
    "            aspect_term = []\n",
    "            aspect_category = []\n",
    "            for a in i.aspect_terms:\n",
    "                aspect_term.append({\n",
    "                    \"term\": a.term,\n",
    "                    \"polarity\": a.polarity,\n",
    "                    \"from\": a.offsets[\"from\"],\n",
    "                    \"to\": a.offsets[\"to\"]\n",
    "                })\n",
    "                # print(a.term, i.text[int(a.offsets['from']): int(a.offsets['to'])])\n",
    "                # if a.term != i.text[int(a.offsets['from']): int(a.offsets['to'])]:\n",
    "                #     print a.term\n",
    "            for c in i.aspect_categories:\n",
    "                aspect_category.append({\n",
    "                    \"category\": c.term,\n",
    "                    \"polarity\": c.polarity\n",
    "                })\n",
    "            opinions = {\"aspect_term\": aspect_term, \"aspect_category\": aspect_category}\n",
    "            ans.append({\n",
    "                \"id\": i.id,\n",
    "                \"text\": i.text,\n",
    "                \"opinions\": opinions\n",
    "            })\n",
    "        import json\n",
    "        json.dump(ans, fr_to)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
