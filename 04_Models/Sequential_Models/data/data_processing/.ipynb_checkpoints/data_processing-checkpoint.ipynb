{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "import json\n",
    "import string\n",
    "import codecs\n",
    "import jieba\n",
    "\n",
    "import torch\n",
    "from pytorch_transformers import *\n",
    "\n",
    "from zhon import hanzi\n",
    "from collections import Counter\n",
    "\n",
    "import xml.etree.ElementTree as ET, getopt, logging, sys, random, re, copy\n",
    "from xml.sax.saxutils import escape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pytorch_transformers.file_utils:https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-chinese-vocab.txt not found in cache, downloading to /var/folders/p5/bh04m0n92x1d64vgj53ntz000000gn/T/tmpb1pwto6q\n",
      "100%|██████████| 109540/109540 [00:00<00:00, 1410068.59B/s]\n",
      "INFO:pytorch_transformers.file_utils:copying /var/folders/p5/bh04m0n92x1d64vgj53ntz000000gn/T/tmpb1pwto6q to cache at /Users/meif/.cache/torch/pytorch_transformers/8a0c070123c1f794c42a29c6904beb7c1b8715741e235bee04aca2c7636fc83f.9b42061518a39ca00b8b52059fd2bede8daa613f8a8671500e518a8c29de8c00\n",
      "INFO:pytorch_transformers.file_utils:creating metadata file for /Users/meif/.cache/torch/pytorch_transformers/8a0c070123c1f794c42a29c6904beb7c1b8715741e235bee04aca2c7636fc83f.9b42061518a39ca00b8b52059fd2bede8daa613f8a8671500e518a8c29de8c00\n",
      "INFO:pytorch_transformers.file_utils:removing temp file /var/folders/p5/bh04m0n92x1d64vgj53ntz000000gn/T/tmpb1pwto6q\n",
      "INFO:pytorch_transformers.tokenization_utils:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-chinese-vocab.txt from cache at /Users/meif/.cache/torch/pytorch_transformers/8a0c070123c1f794c42a29c6904beb7c1b8715741e235bee04aca2c7636fc83f.9b42061518a39ca00b8b52059fd2bede8daa613f8a8671500e518a8c29de8c00\n",
      "INFO:pytorch_transformers.file_utils:https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-chinese-config.json not found in cache, downloading to /var/folders/p5/bh04m0n92x1d64vgj53ntz000000gn/T/tmp8kd25lof\n",
      "100%|██████████| 520/520 [00:00<00:00, 82018.58B/s]\n",
      "INFO:pytorch_transformers.file_utils:copying /var/folders/p5/bh04m0n92x1d64vgj53ntz000000gn/T/tmp8kd25lof to cache at /Users/meif/.cache/torch/pytorch_transformers/8a3b1cfe5da58286e12a0f5d7d182b8d6eca88c08e26c332ee3817548cf7e60a.0c16faba8be66db3f02805c912e4cf94d3c9cffc1f12fa1a39906f9270f76d33\n",
      "INFO:pytorch_transformers.file_utils:creating metadata file for /Users/meif/.cache/torch/pytorch_transformers/8a3b1cfe5da58286e12a0f5d7d182b8d6eca88c08e26c332ee3817548cf7e60a.0c16faba8be66db3f02805c912e4cf94d3c9cffc1f12fa1a39906f9270f76d33\n",
      "INFO:pytorch_transformers.file_utils:removing temp file /var/folders/p5/bh04m0n92x1d64vgj53ntz000000gn/T/tmp8kd25lof\n",
      "INFO:pytorch_transformers.modeling_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-chinese-config.json from cache at /Users/meif/.cache/torch/pytorch_transformers/8a3b1cfe5da58286e12a0f5d7d182b8d6eca88c08e26c332ee3817548cf7e60a.0c16faba8be66db3f02805c912e4cf94d3c9cffc1f12fa1a39906f9270f76d33\n",
      "INFO:pytorch_transformers.modeling_utils:Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 2,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 21128\n",
      "}\n",
      "\n",
      "INFO:pytorch_transformers.file_utils:https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-chinese-pytorch_model.bin not found in cache, downloading to /var/folders/p5/bh04m0n92x1d64vgj53ntz000000gn/T/tmp5sm_eg4z\n",
      "100%|██████████| 411577189/411577189 [01:43<00:00, 3976512.27B/s]\n",
      "INFO:pytorch_transformers.file_utils:copying /var/folders/p5/bh04m0n92x1d64vgj53ntz000000gn/T/tmp5sm_eg4z to cache at /Users/meif/.cache/torch/pytorch_transformers/b1b5e295889f2d0979ede9a78ad9cb5dc6a0e25ab7f9417b315f0a2c22f4683d.929717ca66a3ba9eb9ec2f85973c6398c54c38a4faa464636a491d7a705f7eb6\n",
      "INFO:pytorch_transformers.file_utils:creating metadata file for /Users/meif/.cache/torch/pytorch_transformers/b1b5e295889f2d0979ede9a78ad9cb5dc6a0e25ab7f9417b315f0a2c22f4683d.929717ca66a3ba9eb9ec2f85973c6398c54c38a4faa464636a491d7a705f7eb6\n",
      "INFO:pytorch_transformers.file_utils:removing temp file /var/folders/p5/bh04m0n92x1d64vgj53ntz000000gn/T/tmp5sm_eg4z\n",
      "INFO:pytorch_transformers.modeling_utils:loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-chinese-pytorch_model.bin from cache at /Users/meif/.cache/torch/pytorch_transformers/b1b5e295889f2d0979ede9a78ad9cb5dc6a0e25ab7f9417b315f0a2c22f4683d.929717ca66a3ba9eb9ec2f85973c6398c54c38a4faa464636a491d7a705f7eb6\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-chinese')\n",
    "model = BertModel.from_pretrained('bert-base-chinese')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BERT_CHINESE ='./bert-base-chinese' # your path for model and vocab \n",
    "BERT_CHINESE_VOCAB ='bert-base-chinese-vocab.txt'\n",
    "tokenizer = BertTokenizer.from_pretrained(os.path.join(BERT_CHINESE, BERT_CHINESE_VOCAB))\n",
    "model = BertModel.from_pretrained(BERT_CHINESE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = torch.tensor([tokenizer.encode(\"Here is some text to encode\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[10816,  8311, 13049, 10540,  8229,  9448, 10803]])"
      ]
     },
     "execution_count": 452,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    last_hidden_states = model(input_ids)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.7559, -0.8967,  0.4671,  ...,  0.8768, -0.3557,  0.0315],\n",
       "         [ 0.0102, -0.9331, -0.1013,  ..., -0.0526, -0.3079,  0.1158],\n",
       "         [ 0.5664,  0.0461,  0.1432,  ...,  0.2052, -0.4931,  0.0084],\n",
       "         ...,\n",
       "         [-0.1225, -0.4007,  0.1629,  ..., -0.3553, -0.5007, -0.0392],\n",
       "         [ 0.0384, -0.2626, -0.0595,  ..., -0.2261, -0.2866,  0.2040],\n",
       "         [ 0.5811, -0.1267, -0.2250,  ..., -0.1337, -0.3257,  0.3426]]])"
      ]
     },
     "execution_count": 454,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DATA_DIR_FROM = \"../data/data_orign/SemEval2015\"\n",
    "DATA_DIR_TO = \"../data/data_processed/SemEval2015\"\n",
    "\n",
    "STOPWORDS = codecs.open(\"Frequent_Stopwords_ZH.txt\", encoding=\"utf-8\")\\\n",
    "                  .read()\\\n",
    "                  .replace(\" \", \"\")\\\n",
    "                  .split(\",\")\n",
    "PUNCTUATIONS = set(hanzi.punctuation + string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SENTIMENT_IDX = {'negative': 0, 'positive': 1}\n",
    "IDX_SENTIMENT = {0: 'negative', 1: 'positive'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-5-a04fdb8683c2>, line 126)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-5-a04fdb8683c2>\"\u001b[0;36m, line \u001b[0;32m126\u001b[0m\n\u001b[0;31m    print '%d instances\\n%d distinct aspect terms' % (len(self.corpus), len(self.top_aspect_terms))\u001b[0m\n\u001b[0m                                                 ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "class Category:\n",
    "    '''Category objects contain the term and polarity (i.e., pos, neg, neu, conflict) of the category (e.g., food, price, etc.) of a sentence.'''\n",
    "\n",
    "    def __init__(self, term='', polarity=''):\n",
    "        self.term = term\n",
    "        self.polarity = polarity\n",
    "\n",
    "    def create(self, element):\n",
    "        self.term = element.attrib['category']\n",
    "        self.polarity = element.attrib['polarity']\n",
    "        return self\n",
    "\n",
    "    def update(self, term='', polarity=''):\n",
    "        self.term = term\n",
    "        self.polarity = polarity\n",
    "\n",
    "\n",
    "class Aspect:\n",
    "    '''Aspect objects contain the term (e.g., battery life) and polarity (i.e., pos, neg, neu, conflict) of an aspect.'''\n",
    "\n",
    "    def __init__(self, term, polarity, offsets):\n",
    "        self.term = term\n",
    "        self.polarity = polarity\n",
    "        self.offsets = offsets\n",
    "\n",
    "    def create(self, element):\n",
    "        self.term = element.attrib['term']\n",
    "        self.polarity = element.attrib['polarity']\n",
    "        self.offsets = {'from': str(element.attrib['from']), 'to': str(element.attrib['to'])}\n",
    "        return self\n",
    "\n",
    "    def update(self, term='', polarity=''):\n",
    "        self.term = term\n",
    "        self.polarity = polarity\n",
    "\n",
    "\n",
    "class Instance:\n",
    "    '''An instance is a sentence, modeled out of XML (pre-specified format, based on the 4th task of SemEval 2014).\n",
    "    It contains the text, the aspect terms, and any aspect categories.'''\n",
    "\n",
    "    def __init__(self, element):\n",
    "        self.text = element.find('text').text\n",
    "        self.id = element.get('id')\n",
    "        self.aspect_terms = [Aspect('', '', offsets={'from': '', 'to': ''}).create(e) for es in\n",
    "                             element.findall('aspectTerms') for e in es if\n",
    "                             es is not None]\n",
    "        self.aspect_categories = [Category(term='', polarity='').create(e) for es in element.findall('aspectCategories')\n",
    "                                  for e in es if\n",
    "                                  es is not None]\n",
    "\n",
    "    def get_aspect_terms(self):\n",
    "        return [a.term.lower() for a in self.aspect_terms]\n",
    "\n",
    "    def get_aspect_categories(self):\n",
    "        return [c.term.lower() for c in self.aspect_categories]\n",
    "\n",
    "    def add_aspect_term(self, term, polarity='', offsets={'from': '', 'to': ''}):\n",
    "        a = Aspect(term, polarity, offsets)\n",
    "        self.aspect_terms.append(a)\n",
    "\n",
    "    def add_aspect_category(self, term, polarity=''):\n",
    "        c = Category(term, polarity)\n",
    "        self.aspect_categories.append(c)\n",
    "\n",
    "\n",
    "class Corpus:\n",
    "    '''A corpus contains instances, and is useful for training algorithms or splitting to train/test files.'''\n",
    "\n",
    "    def __init__(self, elements):\n",
    "        self.corpus = [Instance(e) for e in elements]\n",
    "        self.size = len(self.corpus)\n",
    "        self.aspect_terms_fd = fd([a for i in self.corpus for a in i.get_aspect_terms()])\n",
    "        self.top_aspect_terms = freq_rank(self.aspect_terms_fd)\n",
    "        self.texts = [t.text for t in self.corpus]\n",
    "\n",
    "    def echo(self):\n",
    "        print '%d instances\\n%d distinct aspect terms' % (len(self.corpus), len(self.top_aspect_terms))\n",
    "        print 'Top aspect terms: %s' % (', '.join(self.top_aspect_terms[:10]))\n",
    "\n",
    "    def clean_tags(self):\n",
    "        for i in range(len(self.corpus)):\n",
    "            self.corpus[i].aspect_terms = []\n",
    "\n",
    "    def split(self, threshold=0.8, shuffle=False):\n",
    "        '''Split to train/test, based on a threshold. Turn on shuffling for randomizing the elements beforehand.'''\n",
    "        clone = copy.deepcopy(self.corpus)\n",
    "        if shuffle: random.shuffle(clone)\n",
    "        train = clone[:int(threshold * self.size)]\n",
    "        test = clone[int(threshold * self.size):]\n",
    "        return train, test\n",
    "\n",
    "    def write_out(self, filename, instances, short=True):\n",
    "        with open(filename, 'w') as o:\n",
    "            o.write('<sentences>\\n')\n",
    "            for i in instances:\n",
    "                o.write('\\t<sentence id=\"%s\">\\n' % (i.id))\n",
    "                o.write('\\t\\t<text>%s</text>\\n' % fix(i.text))\n",
    "                o.write('\\t\\t<aspectTerms>\\n')\n",
    "                if not short:\n",
    "                    for a in i.aspect_terms:\n",
    "                        o.write('\\t\\t\\t<aspectTerm term=\"%s\" polarity=\"%s\" from=\"%s\" to=\"%s\"/>\\n' % (\n",
    "                            fix(a.term), a.polarity, a.offsets['from'], a.offsets['to']))\n",
    "                o.write('\\t\\t</aspectTerms>\\n')\n",
    "                o.write('\\t\\t<aspectCategories>\\n')\n",
    "                if not short:\n",
    "                    for c in i.aspect_categories:\n",
    "                        o.write('\\t\\t\\t<aspectCategory category=\"%s\" polarity=\"%s\"/>\\n' % (fix(c.term), c.polarity))\n",
    "                o.write('\\t\\t</aspectCategories>\\n')\n",
    "                o.write('\\t</sentence>\\n')\n",
    "            o.write('</sentences>')\n",
    "\n",
    "    def write_out_json(self, filename, instances):\n",
    "        fr_to = open(filename, 'w')\n",
    "        ans = []\n",
    "        for i in instances:\n",
    "            aspect_term = []\n",
    "            aspect_category = []\n",
    "            for a in i.aspect_terms:\n",
    "                aspect_term.append({\n",
    "                    \"term\": a.term,\n",
    "                    \"polarity\": a.polarity,\n",
    "                    \"from\": a.offsets[\"from\"],\n",
    "                    \"to\": a.offsets[\"to\"]\n",
    "                })\n",
    "                # print(a.term, i.text[int(a.offsets['from']): int(a.offsets['to'])])\n",
    "                # if a.term != i.text[int(a.offsets['from']): int(a.offsets['to'])]:\n",
    "                #     print a.term\n",
    "            for c in i.aspect_categories:\n",
    "                aspect_category.append({\n",
    "                    \"category\": c.term,\n",
    "                    \"polarity\": c.polarity\n",
    "                })\n",
    "            opinions = {\"aspect_term\": aspect_term, \"aspect_category\": aspect_category}\n",
    "            ans.append({\n",
    "                \"id\": i.id,\n",
    "                \"text\": i.text,\n",
    "                \"opinions\": opinions\n",
    "            })\n",
    "        import json\n",
    "        json.dump(ans, fr_to)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
